{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *AWS Amazon Web Services*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*En esta primera etapa, hemos decidio utilizar una función Lambda en Amazon Web Services (AWS) encargada de realizar web scraping en la página de la Comisión de Taxis y Limusinas para extraer datos sobre los viajes realizados en la ciudad de Nueva York durante el año 2023. Posteriormente, estos datos son almacenados en un bucket de AWS S3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import boto3\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Definimos la función de manejo de la solicitud\n",
    "def lambda_handler(event, context):\n",
    "    # Configuramos las credenciales de AWS\n",
    "    AWS_ACCESS_KEY_ID = 'AWS_ACCESS_KEY_ID'\n",
    "    AWS_SECRET_ACCESS_KEY = 'AWS_SECRET_ACCESS_KEY'\n",
    "    AWS_REGION = 'AWS_REGION'\n",
    "\n",
    "    # Creamos un cliente de S3\n",
    "    s3_client = boto3.client('s3', aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY, region_name=AWS_REGION)\n",
    "\n",
    "    # Crear carpeta de destino si no existe\n",
    "    carpeta_destino = \"/tmp/0-DataSets/1-BBDDscrapeados\"\n",
    "    if not os.path.exists(carpeta_destino):\n",
    "        os.makedirs(carpeta_destino)\n",
    "\n",
    "    # Obtener la lista de archivos en la carpeta de destino\n",
    "    archivos_locales = os.listdir(carpeta_destino)\n",
    "\n",
    "    # URL de la página web\n",
    "    url = \"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "    # Realizar la solicitud HTTP a la página\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Verificar si la solicitud fue exitosa\n",
    "    if response.status_code == 200:\n",
    "        # Parsear el contenido HTML de la página\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Encontrar todos los enlaces que contienen 'yellow' o 'green' y tienen el formato esperado para 2023\n",
    "        links_pagina = {a['href'] for a in soup.find_all('a', href=True) if re.match(r'^.*(yellow|green)tripdata(2023|2024)-\\d{2}\\.parquet$', a['href'])}\n",
    "\n",
    "        # Combinar enlaces generados dinámicamente con los obtenidos de la página\n",
    "        links_totales = links_pagina.copy()  # Crear una copia para mantener los enlaces originales\n",
    "\n",
    "        meses = [str(num).zfill(2) for num in range(1, 13)]  # Lista de meses como cadenas de dos dígitos\n",
    "\n",
    "        for tipo in ['yellow', 'green']:\n",
    "            links_totales.update({\n",
    "                f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{tipo}_tripdata_{año}-{mes}.parquet\" \n",
    "                for año in ['2023', '2024']  # Iterar sobre los años\n",
    "                for mes in meses\n",
    "            })\n",
    "\n",
    "        # Realizar la descarga de los archivos\n",
    "        for link in sorted(links_totales):  # Iterar sobre los enlaces ordenados alfabéticamente\n",
    "            nombre_archivo = link.split(\"/\")[-1]  # Obtener el nombre del archivo desde la URL\n",
    "            archivo_destino = os.path.join(carpeta_destino, nombre_archivo)  # Construir la ruta completa del archivo destino\n",
    "\n",
    "            if nombre_archivo not in archivos_locales:  # Verificar si el archivo no existe localmente\n",
    "                response = requests.get(link)\n",
    "                if response.status_code == 200:\n",
    "                    with open(archivo_destino, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                    print(f\"Archivo descargado: {nombre_archivo}\")\n",
    "\n",
    "                    # Subir el archivo a S3\n",
    "                    s3_client.upload_file(archivo_destino, 'proceso-webscraping', nombre_archivo)\n",
    "                    print(f\"Archivo subido a S3: {nombre_archivo}\")\n",
    "                else:\n",
    "                    print(f\"No se pudo descargar el archivo desde {link}. Código de estado: {response.status_code}\")\n",
    "            else:\n",
    "                print(f\"El archivo {nombre_archivo} ya existe localmente.\")  # Imprimir un mensaje indicando que el archivo ya existe\n",
    "    else:\n",
    "        print(f\"No se pudo acceder a la página. Código de estado: {response.status_code}\")  # Imprimir un mensaje de error si la solicitud no fue exitosa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Una vez que hemos adquirido los archivos en formato Parquet, recurrimos al empleo de AWS Glue para automatizar el proceso de Extracción, Transformación y Carga (ETL) de los datos utilizando pySpark.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, when, to_date, to_timestamp, hour, round\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from pyspark.context import SparkContext\n",
    "import os\n",
    "# Inicializar el contexto de Glue\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "# Crear un objeto Job\n",
    "job = Job(glueContext)\n",
    "job.init(\"my_job_name\")  # Reemplaza \"my_job_name\" con el nombre deseado para tu job\n",
    "spark = SparkSession.builder.appName(\"Parquet\").getOrCreate()\n",
    "df_yellow_container = []\n",
    "# Iteramos sobre la dirección del contenedor para extraer todos los archivos del 2023\n",
    "for month in range(1, 13):\n",
    "    \n",
    "    # Creamos la variable que contenga el path a cada parquet de yellow taxis\n",
    "    yellow_path = \"s3://webscraping-proceso/yellow_tripdata_2023-{0}.parquet\".format(str(month).zfill(2))\n",
    "    \n",
    "    # Leemos el parquet\n",
    "    df_yellow = spark.read.parquet(yellow_path)\n",
    "    \n",
    "    # Lo agregamos a la lista df_yellow_container\n",
    "    df_yellow_container.append(df_yellow)\n",
    "\n",
    "# Iteramos sobre la dirección del contenedor para extraer todos los archivos del 2024\n",
    "for month in range(1, 2):\n",
    "    \n",
    "    # Creamos la variable que contenga el path a cada parquet de yellow taxis\n",
    "    yellow_path = \"s3://webscraping-proceso/yellow_tripdata_2024-{0}.parquet\".format(str(month).zfill(2))\n",
    "    \n",
    "    # Leemos el parquet\n",
    "    df_yellow = spark.read.parquet(yellow_path)\n",
    "    \n",
    "    # Lo agregamos a la lista df_yellow_container\n",
    "    df_yellow_container.append(df_yellow)\n",
    "\n",
    "# Unir todos los DataFrames en uno solo\n",
    "yellow_concatenado = None\n",
    "for df in df_yellow_container:\n",
    "    if yellow_concatenado is None:\n",
    "        yellow_concatenado = df\n",
    "    else:\n",
    "        yellow_concatenado = yellow_concatenado.union(df)\n",
    "# Agregar una nueva columna 'total_pay' al DataFrame yellow_concatenado que contiene la suma de 'tip_amount' y 'total_amount', limitando 'amount' a 2 decimales\n",
    "yellow_concatenado = yellow_concatenado.withColumn('amount', round(col('tip_amount') + col('total_amount'), 2))\n",
    "# Crear un nuevo DataFrame 'yellowdf' con las columnas seleccionadas\n",
    "yellowdf = yellow_concatenado.select('tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'RatecodeID', 'passenger_count', 'trip_distance', 'amount')\n",
    "# Renombrar las columnas en un nuevo DataFrame\n",
    "yellowdf = yellowdf \\\n",
    "    .withColumnRenamed('tpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('tpep_dropoff_datetime', 'dropoff_datetime')\n",
    "# Eliminamos solo el 7% de las filas donde contiene valores NaN\n",
    "yellowdf = yellowdf.na.drop()\n",
    "# Eliminar filas con valores de pasajeros diferentes de 99 y 0\n",
    "yellowdf = yellowdf.filter((col(\"passenger_count\") != 99) & (col(\"passenger_count\") != 0))\n",
    "\n",
    "# Agregar una columna 'service_type' con el valor 'yellow'\n",
    "yellowdf = yellowdf.withColumn('service_type', lit('yellow'))\n",
    "\n",
    "# Calcular el número total de filas\n",
    "filas_totales_yellow = yellowdf.count()\n",
    "\n",
    "# Calcular el número de filas a tomar para el muestreo\n",
    "porcentaje = 0.20\n",
    "filas_a_tomar_yellow = int(filas_totales_yellow * porcentaje)\n",
    "# Tomar una muestra aleatoria de las filas seleccionadas\n",
    "filas_yellow_seleccionadas = yellowdf.sample(False, porcentaje)\n",
    "df_green_container = []\n",
    "# Iteramos sobre la dirección del contenedor para extraer todos los archivos del 2023\n",
    "for month in range(1, 13):\n",
    "    \n",
    "    # Creamos la variable que contenga el path a cada parquet de green taxis\n",
    "    green_path = \"s3://webscraping-proceso/green_tripdata_2023-{0}.parquet\".format(str(month).zfill(2))\n",
    "    \n",
    "    # Leemos el parquet\n",
    "    df_green = spark.read.parquet(green_path)\n",
    "    \n",
    "    # Lo agregamos a la lista df_green_container\n",
    "    df_green_container.append(df_green)\n",
    "    \n",
    "# Iteramos sobre la dirección del contenedor para extraer todos los archivos del 2024\n",
    "for month in range(1, 2):\n",
    "    \n",
    "    # Creamos la variable que contenga el path a cada parquet de green taxis\n",
    "    green_path = \"s3://webscraping-proceso/green_tripdata_2024-{0}.parquet\".format(str(month).zfill(2))\n",
    "    \n",
    "    # Leemos el parquet\n",
    "    df_green = spark.read.parquet(green_path)\n",
    "    \n",
    "    # Lo agregamos a la lista df_green_container\n",
    "    df_green_container.append(df_green)\n",
    "# Unir todos los DataFrames en uno solo\n",
    "green_concatenado = None\n",
    "for df in df_green_container:\n",
    "    if green_concatenado is None:\n",
    "        green_concatenado = df\n",
    "    else:\n",
    "        green_concatenado = green_concatenado.union(df)\n",
    "# Agregar una nueva columna 'total_pay' al DataFrame green_concatenado que contiene la suma de 'tip_amount' y 'total_amount', limitando 'amount' a 2 decimales\n",
    "green_concatenado = green_concatenado.withColumn('amount', round(col('tip_amount') + col('total_amount'), 2))\n",
    "# Crear un nuevo DataFrame 'greendf' con las columnas seleccionadas\n",
    "greendf = green_concatenado.select('lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'RatecodeID', 'passenger_count', 'trip_distance', 'amount')\n",
    "# Renombrar las columnas en un nuevo DataFrame\n",
    "greendf_renombrado = greendf \\\n",
    "    .withColumnRenamed('lpep_pickup_datetime', 'pickup_datetime') \\\n",
    "    .withColumnRenamed('lpep_dropoff_datetime', 'dropoff_datetime')\n",
    "# Eliminar filas con valores NaN en el DataFrame greendf\n",
    "greendf = greendf.na.drop()\n",
    "# Eliminar filas con valores de pasajeros diferentes de 99 y 0, y trip_distance diferente de 0\n",
    "greendf = greendf.filter((col(\"passenger_count\") != 99) & (col(\"passenger_count\") != 0) & (col(\"trip_distance\") != 0))\n",
    "# Añadir la columna 'service_type' con el valor 'green'\n",
    "greendf = greendf.withColumn('service_type', lit('green'))\n",
    "# Calcular el número de filas total\n",
    "filas_totales = greendf.count()\n",
    "# Calcular el número de filas a tomar para el muestreo\n",
    "porcentaje = 0.20\n",
    "filas_a_tomar = int(filas_totales * porcentaje)\n",
    "# Tomar una muestra aleatoria de las filas seleccionadas\n",
    "filas_green_seleccionadas = greendf.sample(False, porcentaje)\n",
    "# Concatenar los DataFrames de yellow y green taxis\n",
    "taxis = filas_yellow_seleccionadas.union(filas_green_seleccionadas)\n",
    "# Definir el diccionario zones\n",
    "zones = {\n",
    "    'Manhattan': [4, 12, 13, 24, 41, 42, 43, 45, 48, 50, 68, 74, 75, 79, 87, 88, 90, 100, 103, 103, 103, 107, 113, 114, 116, 120, 125,\n",
    "                  127, 128, 137, 140, 141, 142, 143, 144, 148, 151, 152, 153, 158, 161, 162, 163, 164, 166, 170, 186, 194, 202, 209, 211,\n",
    "                  224, 229, 230, 231, 232, 233, 234, 236, 237, 238, 239, 243, 244, 246, 249, 261, 262, 263],\n",
    "    'Brooklyn': [11, 14, 17, 21, 22, 25, 26, 29, 33, 34, 35, 36, 37, 39, 40, 49, 52, 54, 55, 61, 62, 63, 65, 66, 67, 71, 72, 76, 77, 80,\n",
    "                 85, 89, 91, 97, 106, 108, 111, 112, 123, 133, 149, 150, 154, 155, 165, 177, 178, 181, 188, 189, 190, 195, 210, 217, 222,\n",
    "                 225, 227, 228, 255, 256, 257],\n",
    "    'Queens': [2, 7, 8, 9, 10, 15, 16, 19, 27, 28, 30, 38, 53, 56, 56, 64, 70, 73, 82, 83, 86, 92, 93, 95, 96, 98, 101, 102, 117, 121,\n",
    "               122, 124, 129, 130, 131, 132, 134, 135, 138, 139, 145, 146, 157, 160, 171, 173, 175, 179, 180, 191, 192, 193, 196, 197,\n",
    "               198, 201, 203, 205, 207, 215, 216, 218, 219, 223, 226, 252, 253, 258, 260],\n",
    "    'Bronx': [3, 18, 20, 31, 32, 46, 47, 51, 58, 59, 60, 69, 78, 81, 94, 119, 126, 136, 147, 159, 167, 168, 169, 174, 182, 183, 184,\n",
    "              185, 199, 200, 208, 212, 213, 220, 235, 240, 241, 242, 247, 248, 250, 254, 259],\n",
    "    'Staten Island': [5, 6, 23, 44, 84, 99, 109, 110, 115, 118, 156, 172, 176, 187, 204, 206, 214, 221, 245, 251],\n",
    "    'EWR': [1]\n",
    "}\n",
    "# Crear columnas 'pickup_borough' y 'dropoff_borough' utilizando el diccionario zones\n",
    "taxis = taxis.withColumn('pickup_borough', \n",
    "                         when(col('PULocationID').isin(zones['Manhattan']), 'Manhattan')\n",
    "                         .when(col('PULocationID').isin(zones['Brooklyn']), 'Brooklyn')\n",
    "                         .when(col('PULocationID').isin(zones['Queens']), 'Queens')\n",
    "                         .when(col('PULocationID').isin(zones['Bronx']), 'Bronx')\n",
    "                         .when(col('PULocationID').isin(zones['Staten Island']), 'Staten Island')\n",
    "                         .when(col('PULocationID').isin(zones['EWR']), 'EWR')\n",
    "                         .otherwise('Unknown'))\n",
    "\n",
    "taxis = taxis.withColumn('dropoff_borough', \n",
    "                         when(col('DOLocationID').isin(zones['Manhattan']), 'Manhattan')\n",
    "                         .when(col('DOLocationID').isin(zones['Brooklyn']), 'Brooklyn')\n",
    "                         .when(col('DOLocationID').isin(zones['Queens']), 'Queens')\n",
    "                         .when(col('DOLocationID').isin(zones['Bronx']), 'Bronx')\n",
    "                         .when(col('DOLocationID').isin(zones['Staten Island']), 'Staten Island')\n",
    "                         .when(col('DOLocationID').isin(zones['EWR']), 'EWR')\n",
    "                         .otherwise('Unknown'))\n",
    "# Filtrar las filas donde 'pickup_borough' y 'dropoff_borough' no sean 'Unknown'\n",
    "taxis = taxis.filter(~col('pickup_borough').isin(['Unknown']) & ~col('dropoff_borough').isin(['Unknown']))\n",
    "\n",
    "# Cambiar el tipo de dato de 'passenger_count' a entero\n",
    "taxis = taxis.withColumn('passenger_count', col('passenger_count').cast(IntegerType()))\n",
    "# Seleccionar columnas específicas\n",
    "taxisdf = taxis.select('pickup_datetime', 'dropoff_datetime', 'RatecodeID', 'passenger_count', \n",
    "                       'trip_distance', 'amount', 'service_type', 'pickup_borough', 'dropoff_borough')\n",
    "# Separar pickup_datetime en fecha y hora\n",
    "taxisdf = taxisdf.withColumn('pickup_date', to_date(col('pickup_datetime')))\n",
    "taxisdf = taxisdf.withColumn('pickup_hour', hour(col('pickup_datetime')).cast('string'))\n",
    "\n",
    "# Separar dropoff_datetime en fecha y hora\n",
    "taxisdf = taxisdf.withColumn('dropoff_date', to_date(col('dropoff_datetime')))\n",
    "taxisdf = taxisdf.withColumn('dropoff_hour', hour(col('dropoff_datetime')).cast('string'))\n",
    "\n",
    "# Eliminar columnas originales pickup_datetime y dropoff_datetime\n",
    "taxisdf = taxisdf.drop('pickup_datetime', 'dropoff_datetime')\n",
    "# Especificar la ruta de salida en S3 donde se guardará el archivo Parquet\n",
    "ruta_salida = \"s3://taxisclean/taxis_NYC_2023.parquet\"\n",
    "# Especificamos que queremos un solo archivo parquet y lo guardamos en el bucket\n",
    "taxisdf.coalesce(1).write.parquet(ruta_salida, mode='overwrite')\n",
    "# Detener sesion de Spark\n",
    "spark.stop()\n",
    "# Finalizar el job\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Tras finalizar la procesamiento de los datos, se procede a guardar el archivo resultante en formato Parquet en un bucket de AWS S3. No obstante, se detecta que el archivo se almacena con un nombre predeterminado, generado automáticamente durante el proceso de guardado utilizando Apache Spark con compresión Snappy. Para solventar este inconveniente, se implementa una función Lambda adicional que se encarga de renombrar el archivo antes de almacenarlo en un bucket destinado a alimentar nuestro Data Warehouse.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # Nombre del bucket y ruta del archivo original\n",
    "    source_bucket_name = 'tu_bucket_de_entrada'\n",
    "    destination_bucket_name = 'tu_bucket_de_salida'\n",
    "    source_folder_name = 'tu_carpeta/'\n",
    "    \n",
    "    # Creamos una instancia del cliente de S3 con tus credenciales de acceso\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        region_name='tu_region',\n",
    "        aws_access_key_id='tu_access_key',\n",
    "        aws_secret_access_key='tu_secret_key'\n",
    "    )\n",
    "    \n",
    "    # Obtenemos la lista de objetos en la carpeta especificada\n",
    "    response = s3_client.list_objects_v2(\n",
    "        Bucket=source_bucket_name,\n",
    "        Prefix=source_folder_name\n",
    "    )\n",
    "    \n",
    "    # Renombramos y copiamos cada archivo que termine en \".parquet\" al nuevo bucket\n",
    "    for obj in response.get('Contents', []):\n",
    "        file_key = obj['Key']\n",
    "        if file_key.endswith('.parquet'):\n",
    "            new_file_key = 'nuevo_nombre.parquet'\n",
    "            s3_client.copy_object(\n",
    "                Bucket=destination_bucket_name,\n",
    "                CopySource={'Bucket': source_bucket_name, 'Key': file_key},\n",
    "                Key=new_file_key\n",
    "            )\n",
    "            s3_client.delete_object(\n",
    "                Bucket=source_bucket_name,\n",
    "                Key=file_key\n",
    "            )\n",
    "    \n",
    "    # Eliminamos todos los objetos dentro de la carpeta original\n",
    "    for obj in response.get('Contents', []):\n",
    "        s3_client.delete_object(\n",
    "            Bucket=source_bucket_name,\n",
    "            Key=obj['Key']\n",
    "        )\n",
    "\n",
    "    # Eliminamos la carpeta original después de copiar y eliminamos todos los archivos\n",
    "    s3_client.delete_object(\n",
    "        Bucket=source_bucket_name,\n",
    "        Key=source_folder_name\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'Archivos renombrados, copiados al nuevo bucket y carpeta eliminada exitosamente'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Una vez que el archivo está en el bucket que abastece a AWS Athena, podemos realizar consultas SQL para obtener información de los datos almacenados en el Data Warehouse.*\n",
    "*A medida que la base de datos se renueva, nuestro Data Warehouse se actualiza automáticamente mediante una función Lambda. Esta función se encarga de tomar los nuevos registros de viajes y almacenarlos en el Data Warehouse.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
